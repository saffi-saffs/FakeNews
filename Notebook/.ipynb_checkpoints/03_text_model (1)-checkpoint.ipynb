{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib tensorflow nltk re sklearn \n",
    "pip install tensorflow-gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VUS-M4pXXspJ",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\.virtualenvs\\Google-9yx6rxY-\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input,GlobalMaxPool1D,Dropout\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords #corpus is collection of text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"error\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ehpMEkMvXspP",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load train data\n",
    "train = pd.read_csv('D:\\\\Google\\\\Models\\\\fakeNewsDataset\\\\train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GkEDwwuUXspR",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop the Nan Values\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VSTQ0CunXspR",
    "outputId": "5ea5992b-8eab-4d93-f92b-f551528aab55",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_26936\\4237079017.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train['text'].fillna( train['title'],inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text         0\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace na\n",
    "train['text'].fillna( train['title'],inplace=True)\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PBmeQytEXspS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get the Depndent feature\n",
    "X_train=train.drop('label',axis=1)\n",
    "y_train=train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "s7J4I-RRXspT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set vocabulary size\n",
    "vo_size=500\n",
    "messages=X_train.copy()\n",
    "messages.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "ujWxPnfeXspU",
    "outputId": "44a74509-f98b-43ff-8167-6d638bbd3b08",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 119 / 20800\r"
     ]
    }
   ],
   "source": [
    "#dataset Preprocessing\n",
    "nltk.download('stopwords')\n",
    "ps =PorterStemmer()\n",
    "corpus = []\n",
    "for i in range(0, len(messages)):\n",
    "    print(\"Status: %s / %s\" %(i, len(messages)), end=\"\\r\")\n",
    "    review = re.sub('[^a-zA-Z]', ' ',messages['text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "\n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6FUZ9DDwUvM",
    "outputId": "8f1ec37c-4d76-4f5e-d4cb-89e6c3cd145e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n",
      "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n",
      "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Initialize Word2Vec model with an initial corpus\n",
    "word2vec_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# Function to process data in batches\n",
    "def process_data_in_batches(data, batch_size):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i+batch_size]\n",
    "\n",
    "# Train the Word2Vec model in batches\n",
    "for batch_data in process_data_in_batches(corpus, batch_size=1000):\n",
    "    word2vec_model.build_vocab(batch_data, update=True)\n",
    "    word2vec_model.train(batch_data, total_examples=word2vec_model.corpus_count, epochs=5)\n",
    "\n",
    "# Get Word2Vec embeddings\n",
    "X_train_word2vec = []\n",
    "for sentence in corpus:\n",
    "    word2vec_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec_model.wv.key_to_index:\n",
    "            word2vec_sentence.append(word2vec_model.wv[word])\n",
    "        else:\n",
    "            word2vec_sentence.append(np.zeros(100))  # Replace OOV words with zero vectors\n",
    "    X_train_word2vec.append(word2vec_sentence)\n",
    "\n",
    "# Pad sequences\n",
    "sent_length = 1000\n",
    "X_train_word2vec = pad_sequences(X_train_word2vec, padding='pre', maxlen=sent_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LfOtotwAQVP"
   },
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# Get Word2Vec embeddings\n",
    "X_train_word2vec = []\n",
    "for sentence in corpus:\n",
    "    word2vec_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec_model.wv.key_to_index:\n",
    "            word2vec_sentence.append(word2vec_model.wv[word])\n",
    "    X_train_word2vec.append(word2vec_sentence)\n",
    "\n",
    "# Pad sequences\n",
    "sent_length = 1000\n",
    "X_train_word2vec = pad_sequences(X_train_word2vec, padding='pre', maxlen=sent_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrGaNcrWbwJl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZFSZx_9b0Y9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UYnD4SzZXspW"
   },
   "outputs": [],
   "source": [
    "# model 1 build\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word2vec_model.wv), output_dim=word2vec_model.vector_size, input_length=sent_length, weights=[word2vec_model.wv.vectors], trainable=False))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_k7DmnAXspW"
   },
   "outputs": [],
   "source": [
    "# Plot title model\n",
    "plot_model(model, to_file='model_plot2.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rz9yjokyXspX"
   },
   "outputs": [],
   "source": [
    "# check shape\n",
    "len(embedded_doc),y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XeP_u_tXspY"
   },
   "outputs": [],
   "source": [
    "# final data for NN\n",
    "X_final=np.array(embedded_doc)\n",
    "y_final=np.array(y_train)\n",
    "X_final.shape,y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCdB5qrzXspY"
   },
   "outputs": [],
   "source": [
    "# train model 1\n",
    "history = model.fit(X_final,y_final, validation_split=0.2, epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpgrQbEKXspZ"
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdWUDgpwXspZ"
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l17Fa6UP5cUp"
   },
   "outputs": [],
   "source": [
    "model.save('/content/drive/MyDrive/modelWeights/stm1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a76h4eswXspZ"
   },
   "outputs": [],
   "source": [
    "# model 2 build\n",
    "model = Sequential()\n",
    "model.add(Embedding(vo_size,embedding_vector_feature,input_length=sent_length))\n",
    "model.add(LSTM(100, return_sequences=True,name='lstm_layer'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_9vvR8fXspa"
   },
   "outputs": [],
   "source": [
    "# Plot title model\n",
    "plot_model(model, to_file='model_plot3.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NAn-t5_Xspa"
   },
   "outputs": [],
   "source": [
    "# train model 2\n",
    "history = model.fit(X_final,y_final, validation_split=0.2, epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_mKaZb6Xspb"
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0vbpM_gXspe"
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMcm4hywXspf"
   },
   "outputs": [],
   "source": [
    "# load test\n",
    "test = pd.read_csv('Data\\\\test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFrUW9LjXspf"
   },
   "outputs": [],
   "source": [
    "# check na in test\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02wPEz45Xspg"
   },
   "outputs": [],
   "source": [
    "# Replace na\n",
    "test['text'] = test['text'].replace(np.nan, test['title'])\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4XUWkNqXsph"
   },
   "outputs": [],
   "source": [
    "# prepare test data for NN\n",
    "X_test=test\n",
    "messages=X_test.copy()\n",
    "messages.reset_index(inplace=True)\n",
    "ps =PorterStemmer()\n",
    "corpus = []\n",
    "for i in range(0, len(messages)):\n",
    "    print(\"Status: %s / %s\" %(i, len(messages)), end=\"\\r\")\n",
    "    review = re.sub('[^a-zA-Z]', ' ',messages['text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "\n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "onehot_rep = [one_hot(words, vo_size) for words in corpus]\n",
    "embedded_doc=pad_sequences(onehot_rep, padding='pre', maxlen=sent_length)\n",
    "X_test_final=np.array(embedded_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8KRyi4RXsph"
   },
   "outputs": [],
   "source": [
    "# predict final\n",
    "y_pred_final=model.predict_classes(X_test_final)\n",
    "y_pred_final = pd.DataFrame(y_pred_final)\n",
    "submit = pd.concat([test['id'].reset_index(drop=True), y_pred_final], axis=1)\n",
    "submit.rename(columns={ submit.columns[1]: \"label\" }, inplace = True)\n",
    "submit.to_csv('submit_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEFfpGeWXspi"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_weights(\"model_text.h5\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Google-9yx6rxY-",
   "language": "python",
   "name": "google-9yx6rxy-"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
