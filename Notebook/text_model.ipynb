{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (58.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from matplotlib) (6.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.26.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (7.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.4)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow pandas numpy matplotlib nltk scikit-learn gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\admin\\.virtualenvs\\google-9yx6rxy-\\lib\\site-packages (2.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VUS-M4pXXspJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input,GlobalMaxPool1D,Dropout\n",
    "from keras.utils import plot_model\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords #corpus is collection of text\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ehpMEkMvXspP",
    "outputId": "12d22b00-4388-463d-8f66-ef8001c70cb8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load train data\n",
    "train = pd.read_csv('D:\\\\Google\\\\Models\\\\fakeNewsDataset\\\\train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GkEDwwuUXspR",
    "outputId": "db1a5f59-8aad-4442-ba7d-9c76995ced8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop the Nan Values\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VSTQ0CunXspR",
    "outputId": "2d9e0c90-4530-48e0-8f90-d506f24bfbff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text         0\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace na\n",
    "train['text'].fillna( train['title'],inplace=True)\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "PBmeQytEXspS"
   },
   "outputs": [],
   "source": [
    "#Get the Depndent feature\n",
    "X_train=train.drop('label',axis=1)\n",
    "y_train=train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "s7J4I-RRXspT"
   },
   "outputs": [],
   "source": [
    "# set vocabulary size\n",
    "vo_size=500\n",
    "messages=X_train.copy()\n",
    "messages.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the text data\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if word not in stopwords.words('english')]\n",
    "    return ' '.join(text)\n",
    "\n",
    "train['processed_text'] = train['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set vocabulary size\n",
    "vo_size = 500\n",
    "DIM=100\n",
    "\n",
    "# Tokenize the corpus\n",
    "sentences_list = [text.split() for textt in train['processed_text'].tolist()]\n",
    "\n",
    "# Train Word2Vec model using the tokenized corpus\n",
    "word2vec_model = Word2Vec(sentences=sentences_list, vector_size=DIM, window=10, min_count=1)\n",
    "\n",
    "\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences_list)\n",
    "x=tokenizer.texts_to_sequences(sentences_list)\n",
    "\n",
    "\n",
    "# Convert the entire corpus into list of embeddings\n",
    "\n",
    "embedded_doc = []\n",
    "for sequence in x:\n",
    "    embedded_sequence = []\n",
    "    for word_index in sequence:\n",
    "        word = tokenizer.index_word[word_index]\n",
    "        if word in word2vec_model.wv:\n",
    "            embedded_sequence.append(word2vec_model.wv[word])\n",
    "    embedded_doc.append(embedded_sequence)\n",
    "# Prepare X_final and y_final for Neural Network\n",
    "embedded_doc = pad_sequences(embedded_doc, padding='pre', maxlen=1000)\n",
    "X_final = np.array(embedded_doc)\n",
    "y_final = np.array(y_train)\n",
    "\n",
    "\n",
    "vocab_size=len(tokenizer.word_index)+1\n",
    "vocab=tokenizer.word_index\n",
    "# Model Building\n",
    "embedding_vector_feature = 100\n",
    "model = Sequential()\n",
    "\n",
    "# Add embedding layer with Word2Vec weights\n",
    "model.add(Embedding(input_dim=len(word2vec_model.wv.index_to_key), output_dim=embedding_vector_feature, weights=[word2vec_model.wv.vectors], input_length=1000, trainable=False))\n",
    "\n",
    "# Add LSTM layer\n",
    "model.add(LSTM(200))\n",
    "\n",
    "# Add Dense layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Plot Model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(X_final, y_final, validation_split=0.2, epochs=10, batch_size=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "# Train Word2Vec model\n",
    "corpus = [text.split() for text in train['processed_text']]\n",
    "word2vec_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# Convert text data to sequences of Word2Vec embeddings and pad sequences\n",
    "max_sequence_length = 1000\n",
    "X_train_word2vec = []\n",
    "for text in corpus:\n",
    "    word2vec_sequence = []\n",
    "    for word in text:\n",
    "        if word in word2vec_model.wv:\n",
    "            word2vec_sequence.append(word2vec_model.wv[word])\n",
    "    X_train_word2vec.append(word2vec_sequence)\n",
    "\n",
    "X_train_word2vec = pad_sequences(X_train_word2vec, padding='pre', maxlen=max_sequence_length)\n",
    "\n",
    "# Prepare target variable\n",
    "y_train = train['label']\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_word2vec, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136522\n"
     ]
    }
   ],
   "source": [
    "print(len(word2vec_model.wv.key_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model.wv.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have already trained the Word2Vec model (word2vec_model) and defined max_sequence_length\n",
    "\n",
    "# Get the vocabulary size and vector dimension\n",
    "vocab_size = len(word2vec_model.wv.key_to_index)+1\n",
    "vector_dim = 100\n",
    "\n",
    "# Build the Sequential model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=vector_dim, trainable=False))\n",
    "\n",
    "\n",
    "\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 6.20 GiB for an array with shape (16640, 1000, 100) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:91\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to an `EagerTensor`.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mNote that this function could return cached copies of created constants for\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m  TypeError: if `dtype` is not compatible with the type of t.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m     88\u001b[0m   \u001b[38;5;66;03m# Make a copy explicitly because the EagerTensor might share the underlying\u001b[39;00m\n\u001b[0;32m     89\u001b[0m   \u001b[38;5;66;03m# memory with the input array. Without this copy, users will be able to\u001b[39;00m\n\u001b[0;32m     90\u001b[0m   \u001b[38;5;66;03m# modify the EagerTensor after its creation by changing the input array.\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m   value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, ops\u001b[38;5;241m.\u001b[39mEagerTensor):\n\u001b[0;32m     93\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 6.20 GiB for an array with shape (16640, 1000, 100) and data type int32"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save('/content/drive/MyDrive/modelWeights/fake_news_classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6FUZ9DDwUvM",
    "outputId": "3d399046-759e-48b4-f476-a27553ae1b39"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "YGuZKId6cLmq",
    "outputId": "03f72dc1-0232-4eab-a43a-0b31b96de5c6"
   },
   "outputs": [],
   "source": [
    "# Embed all words in the entire corpus\n",
    "all_words = [word for sentence in corpus for word in sentence]\n",
    "word_embeddings = [word2vec_model.wv[word] if word in word2vec_model.wv else np.zeros(100) for word in all_words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LfOtotwAQVP"
   },
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# Get Word2Vec embeddings\n",
    "X_train_word2vec = []\n",
    "for sentence in corpus:\n",
    "    word2vec_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec_model.wv.key_to_index:\n",
    "            word2vec_sentence.append(word2vec_model.wv[word])\n",
    "    X_train_word2vec.append(word2vec_sentence)\n",
    "\n",
    "# Pad sequences\n",
    "sent_length = 1000\n",
    "X_train_word2vec = pad_sequences(X_train_word2vec, padding='pre', maxlen=sent_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrGaNcrWbwJl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JZFSZx_9b0Y9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UYnD4SzZXspW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 1 build\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(word2vec_model.wv), \n",
    "                    output_dim=word2vec_model.vector_size, \n",
    "                    weights=[word2vec_model.wv.vectors], \n",
    "                    trainable=False))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_k7DmnAXspW"
   },
   "outputs": [],
   "source": [
    "# Plot title model\n",
    "plot_model(model, to_file='model_plot2.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rz9yjokyXspX"
   },
   "outputs": [],
   "source": [
    "# check shape\n",
    "len(embedded_doc),y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XeP_u_tXspY"
   },
   "outputs": [],
   "source": [
    "# final data for NN\n",
    "X_final=np.array(embedded_doc)\n",
    "y_final=np.array(y_train)\n",
    "X_final.shape,y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCdB5qrzXspY"
   },
   "outputs": [],
   "source": [
    "# train model 1\n",
    "history = model.fit(X_final,y_final, validation_split=0.2, epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpgrQbEKXspZ"
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fdWUDgpwXspZ"
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l17Fa6UP5cUp"
   },
   "outputs": [],
   "source": [
    "model.save('/content/drive/MyDrive/modelWeights/stm1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a76h4eswXspZ"
   },
   "outputs": [],
   "source": [
    "# model 2 build\n",
    "model = Sequential()\n",
    "model.add(Embedding(vo_size,embedding_vector_feature,input_length=sent_length))\n",
    "model.add(LSTM(100, return_sequences=True,name='lstm_layer'))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_9vvR8fXspa"
   },
   "outputs": [],
   "source": [
    "# Plot title model\n",
    "plot_model(model, to_file='model_plot3.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8NAn-t5_Xspa"
   },
   "outputs": [],
   "source": [
    "# train model 2\n",
    "history = model.fit(X_final,y_final, validation_split=0.2, epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_mKaZb6Xspb"
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0vbpM_gXspe"
   },
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMcm4hywXspf"
   },
   "outputs": [],
   "source": [
    "# load test\n",
    "test = pd.read_csv('Data\\\\test.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFrUW9LjXspf"
   },
   "outputs": [],
   "source": [
    "# check na in test\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02wPEz45Xspg"
   },
   "outputs": [],
   "source": [
    "# Replace na\n",
    "test['text'] = test['text'].replace(np.nan, test['title'])\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4XUWkNqXsph"
   },
   "outputs": [],
   "source": [
    "# prepare test data for NN\n",
    "X_test=test\n",
    "messages=X_test.copy()\n",
    "messages.reset_index(inplace=True)\n",
    "ps =PorterStemmer()\n",
    "corpus = []\n",
    "for i in range(0, len(messages)):\n",
    "    print(\"Status: %s / %s\" %(i, len(messages)), end=\"\\r\")\n",
    "    review = re.sub('[^a-zA-Z]', ' ',messages['text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "\n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "onehot_rep = [one_hot(words, vo_size) for words in corpus]\n",
    "embedded_doc=pad_sequences(onehot_rep, padding='pre', maxlen=sent_length)\n",
    "X_test_final=np.array(embedded_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T8KRyi4RXsph"
   },
   "outputs": [],
   "source": [
    "# predict final\n",
    "y_pred_final=model.predict_classes(X_test_final)\n",
    "y_pred_final = pd.DataFrame(y_pred_final)\n",
    "submit = pd.concat([test['id'].reset_index(drop=True), y_pred_final], axis=1)\n",
    "submit.rename(columns={ submit.columns[1]: \"label\" }, inplace = True)\n",
    "submit.to_csv('submit_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BEFfpGeWXspi"
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save_weights(\"model_text.h5\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
